---
title: 2019-5-26 Hadoop 
tags: Hadoop
grammar_cjkRuby: true
---
#### 1、hadoop 安装
   1、首先建hadoop
	  passwd hadoop 输入修改的密码
   2、将hadoop用户添加到sudo中
      vi /etc/sudoers
	  root    ALL=(ALL)     ALL
	  hadoop  ALL=(ALL)     ALL
   3、在/opt下建software和modules文件夹，并修改为hadoop的用户及组
     chown hadoop:hadoop modules
	 chown hadoop:hadoop software
   4、hadoop用户做免密码登录
   5、解压hadoop安装包 
   6、修改hadoop-env.sh
       /opt/modules/hadoop-2.7.2/etc/hadoop/hadoop-env.sh
	   修改JAVA_HOME为你正确的JAVA_HOME
   7、将HADOOP添加到环境变量中
      vim /etc/profile.d/hadoop.sh
	  
	  export HADOOP_HOME=/opt/modules/hadoop-2.7.2
	  export PATH=$PATH:$HADOOP_HOME/bin
	8、source /etc/profile.d/hadoop.sh 
	9、将hadoop的解压文件及配置环境文件分发到其他主机
	   scp -r /opt/modules/hadoop-2.7.2/ root@linux02:/opt/modules/
	   scp /etc/profile.d/hadoop.sh root@linux02:/etc/profile.d/
	10、将/opt下的所有文件的属主属组修改为hadoop
	   chown -r hadoop:hadoop /opt
--------------------------------------------
配置hdfs
  /opt/modules/hadoop-2.7.2/etc/hadoop
	
    1、vim core-site.xml
	  <configuration>
			<!--指定namenode的主机和端口-->
			<property>
					<name>fs.defaultFS</name>
					<value>hdfs://linux01:9000</value>
			</property>
			<!--hadoop存储的文件所在的目录-->
			<property>
					<name>hadoop.tmp.dir</name>
					<value>/opt/modules/hadoop-2.7.2/data/tmp</value>
			</property>
	</configuration>
	 2、修改slaves  ：指定datanode所在的节点
	    vim slaves
		datanode的主机名
     3、配置hdfs-site.xml
	    <configuration>
				<!--hdfs的副本数-->
				<property>
						<name>dfs.replication</name>
						<value>3</value>
				</property>
				<!--secondaryNamed的http及https端口-->
				<property>
						 <name>dfs.namenode.secondary.http-address</name>
						 <value>linux02:50090</value>
				</property>
				<property>
						<name>dfs.namenode.secondary.https-address</name>
						<value>linux02:50091</value>
				</property>
		</configuration>
	 4、将linux01上的hadoop的配置分发到其他主机
	    scp /opt/modules/hadoop-2.7.2/etc/hadoop/* hadoop@linux03:/opt/modules/hadoop-2.7.2/etc/hadoop/
     5、在namenode节点格式化namenode
	    cd /opt/modules/hadoop-2.7.2
		bin/hdfs namenode -format
	 6、启动hdfs
	     sbin/start-dfs.sh
	 7、如果上面过程出问题，看日志解决问题
	 8、http://192.168.25.171:50070
	 9、简单操作：
	    1、在hdfs上建文件夹
		   bin/hdfs dfs -mkdir -p /user/test
		2、将文件传到hdfs上
			bin/hdfs dfs -put /opt/software/jdk-8u171-linux-x64.tar.gz /user/test


1、准备3台虚拟机
2、配置好网络，配置好主机名及免密码登录
3、装好jdk，并配置好jdk的环境变量
4、建好hadoop用户,并配置hadoop用户的免密码登录
5、解压hadoop，并配置hadoop-env.sh
6、将解压配置好的文件分发到其他主机
   修改/opt的属主及属组为hadoop

	   
	   
	   
	